{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain and Hugging Face\n",
    "\n",
    "For more info you can see [LanChain Course](https://www.python-engineer.com/posts/langchain-crash-course/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages\n",
    "!pip -q install langchain huggingface_hub transformers sentence_transformers\n",
    "!pip -q install accelerate bitsandbytes torch pytorch pytorch-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use hugging face hub (online model)\n",
    "\n",
    "import os\n",
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "\n",
    "template = \"\"\"Question: {question} \\nAnswer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=HuggingFaceHub(repo_id=\"databricks/dolly-v2-3b\",\n",
    "                                                       huggingfacehub_api_token=os.environ[\"HUG_FACE\"], \n",
    "                                                       model_kwargs={\"temperature\":0, \"max_length\":64}))\n",
    "\n",
    "print(llm_chain.run(\"What is the capital of France?\"))\n",
    "print(llm_chain.run(\"What area is best for growing wine in France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = 'google/flan-t5-large'# go for a smaller model if you dont have the VRAM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=False)\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n",
      "hebron\n",
      "The capital of England is London. London is the capital of England. So the answer is London.\n",
      "The capital of Israel is Jerusalem. Jerusalem is the capital of Israel. So the answer is Jerusalem.\n",
      "The equation is 2**3 = 32. The answer: 3.\n"
     ]
    }
   ],
   "source": [
    "# use local model\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(local_llm(\"solve equation: 2*x + 1 = 5\"))\n",
    "print(local_llm(\"What is the capital of Israel?\"))\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=local_llm)\n",
    "print(llm_chain.run(\"What is the capital of England?\"))\n",
    "print(llm_chain.run(\"What is the capital of Israel?\"))\n",
    "print(llm_chain.run(\"Solve the equation 2**3-2?\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
