{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain and Hugging Face  \n",
    "\n",
    "[Hugging Face](https://huggingface.co/) Models can be used in a local and online modes.\n",
    "When used in a local mode, models can be fine-tuned (retrained) or extended by embedding in a vector database.\n",
    "For more info you can see [LanChain Course](https://www.python-engineer.com/posts/langchain-crash-course/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-w1oe7huc/pytorch_7d172afd1aea426fb8ac8cf1460eac50/setup.py\", line 15, in <module>\n",
      "  \u001b[31m   \u001b[0m     raise Exception(message)\n",
      "  \u001b[31m   \u001b[0m Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "!pip -q install langchain huggingface_hub transformers sentence_transformers\n",
    "!pip -q install accelerate bitsandbytes torch pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HUG_FACE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/dimastatz.github.io/courses/langchain/examples.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bfuzzy-barnacle-w65r75g9jj29ppp/workspaces/dimastatz.github.io/courses/langchain/examples.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m template \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mQuestion: \u001b[39m\u001b[39m{question}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAnswer: Let\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms think step by step.\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bfuzzy-barnacle-w65r75g9jj29ppp/workspaces/dimastatz.github.io/courses/langchain/examples.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m prompt \u001b[39m=\u001b[39m PromptTemplate(template\u001b[39m=\u001b[39mtemplate, input_variables\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bfuzzy-barnacle-w65r75g9jj29ppp/workspaces/dimastatz.github.io/courses/langchain/examples.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m llm_chain \u001b[39m=\u001b[39m LLMChain(prompt\u001b[39m=\u001b[39mprompt, llm\u001b[39m=\u001b[39mHuggingFaceHub(repo_id\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdatabricks/dolly-v2-3b\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bfuzzy-barnacle-w65r75g9jj29ppp/workspaces/dimastatz.github.io/courses/langchain/examples.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m                                                        huggingfacehub_api_token\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39;49menviron[\u001b[39m\"\u001b[39;49m\u001b[39mHUG_FACE\u001b[39;49m\u001b[39m\"\u001b[39;49m], \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bfuzzy-barnacle-w65r75g9jj29ppp/workspaces/dimastatz.github.io/courses/langchain/examples.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m                                                        model_kwargs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m64\u001b[39m}))\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bfuzzy-barnacle-w65r75g9jj29ppp/workspaces/dimastatz.github.io/courses/langchain/examples.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(llm_chain\u001b[39m.\u001b[39mrun(\u001b[39m\"\u001b[39m\u001b[39mWhat is the capital of France?\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bfuzzy-barnacle-w65r75g9jj29ppp/workspaces/dimastatz.github.io/courses/langchain/examples.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(llm_chain\u001b[39m.\u001b[39mrun(\u001b[39m\"\u001b[39m\u001b[39mWhat area is best for growing wine in France?\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/os.py:680\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodekey(key)]\n\u001b[1;32m    678\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HUG_FACE'"
     ]
    }
   ],
   "source": [
    "# use hugging face hub (online model)\n",
    "\n",
    "import os\n",
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "\n",
    "template = \"\"\"Question: {question} \\nAnswer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=HuggingFaceHub(repo_id=\"databricks/dolly-v2-3b\",\n",
    "                                                       huggingfacehub_api_token=os.environ[\"HUG_FACE\"], \n",
    "                                                       model_kwargs={\"temperature\":0, \"max_length\":64}))\n",
    "\n",
    "print(llm_chain.run(\"What is the capital of France?\"))\n",
    "print(llm_chain.run(\"What area is best for growing wine in France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = 'google/flan-t5-large'# go for a smaller model if you dont have the VRAM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=False)\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n",
      "hebron\n",
      "The capital of England is London. London is the capital of England. So the answer is London.\n",
      "The capital of Israel is Jerusalem. Jerusalem is the capital of Israel. So the answer is Jerusalem.\n",
      "The equation is 2**3 = 32. The answer: 3.\n"
     ]
    }
   ],
   "source": [
    "# use local model\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(local_llm(\"solve equation: 2*x + 1 = 5\"))\n",
    "print(local_llm(\"What is the capital of Israel?\"))\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=local_llm)\n",
    "print(llm_chain.run(\"What is the capital of England?\"))\n",
    "print(llm_chain.run(\"What is the capital of Israel?\"))\n",
    "print(llm_chain.run(\"Solve the equation 2**3-2?\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
